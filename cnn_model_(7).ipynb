{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-6.m81",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "cnn-model (7).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramapriyan912001/Sarcasm-Detection/blob/main/cnn_model_(7).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFOGWVPW8t70"
      },
      "source": [
        "## Reddit Sarcasm Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5emMNA6x8t73"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw4BK4rh8t73",
        "tags": []
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim import models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, AveragePooling1D, MaxPooling1D, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from gensim.test.utils import datapath\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec\n",
        "import math\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam, SGD\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, initializers, regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBUBUIa38t8D",
        "tags": [],
        "outputId": "f430be6e-e110-4283-93cd-d3a0228f2098"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "porter_stemmer = PorterStemmer()\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "word_tokenizer2 = WordPunctTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "-gxvDHRW5N2C"
      },
      "source": [
        "data_cnn = pd.read_csv(\"./with_sentiment_scores.csv\")\n",
        "data_cnn = data_cnn.iloc[:math.floor(len(data_cnn)), :]\n",
        "data_cnn[\"comment\"] = data_cnn[\"comment\"].astype(str)\n",
        "data_cnn[\"parent_comment\"] = data_cnn[\"parent_comment\"].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6tMerA05N2C"
      },
      "source": [
        "## Define a function that splits training set into just sarcasm and just non-sarcasm\n",
        "def split_training_dataset_into_separate_labels(training_dataset):\n",
        "    sarcasm = training_dataset[training_dataset['label'] == 1]\n",
        "    non_sarcasm = training_dataset[training_dataset['label'] == 0]\n",
        "    return sarcasm, non_sarcasm\n",
        "\n",
        "## Define function to engineer features for model such as subreddit history and author history\n",
        "def feature_history(training_dataset, col):\n",
        "    history_sarcasm = {}\n",
        "    history_non_sarcasm = {}\n",
        "    \n",
        "    total_comments_by_feature_history = {}\n",
        "    proportion_sarcasm_by_feature_history = {}\n",
        "    \n",
        "    for index, row in training_dataset.iterrows():\n",
        "        if int(row['label']) == 1:\n",
        "            if row[col] not in history_sarcasm:\n",
        "                history_sarcasm[row[col]] = 0\n",
        "                history_non_sarcasm[row[col]] = 0\n",
        "            history_sarcasm[row[col]] += 1\n",
        "    \n",
        "        elif int(row['label']) == 0:\n",
        "            if row[col] not in history_non_sarcasm:\n",
        "                history_non_sarcasm[row[col]] = 0\n",
        "                history_sarcasm[row[col]] = 0\n",
        "            history_non_sarcasm[row[col]] += 1\n",
        "    \n",
        "    for val in history_sarcasm.keys():\n",
        "        num_sarcasm = history_sarcasm[val]\n",
        "        num_non_sarcasm = history_non_sarcasm[val]\n",
        "        total_comments = num_sarcasm + num_non_sarcasm\n",
        "        sarcasm_proportion = num_sarcasm/total_comments\n",
        "        \n",
        "        proportion_sarcasm_by_feature_history[val] = sarcasm_proportion\n",
        "        total_comments_by_feature_history[val] = total_comments\n",
        "    \n",
        "    return proportion_sarcasm_by_feature_history, total_comments_by_feature_history\n",
        "\n",
        "\n",
        "\n",
        "## Define function to prepare training dataset\n",
        "\n",
        "def add_feature_history_to_train(train_dataset, col):\n",
        "    (proportion_history, total_comments_history) = feature_history(train_dataset, col)\n",
        "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
        "    total_col = \"total_num_comments_by_\" + col\n",
        "    \n",
        "    train_dataset[proportion_col] = train_dataset[col].apply(lambda x: proportion_history[x])\n",
        "    train_dataset[total_col] = train_dataset[col].apply(lambda x: total_comments_history[x])\n",
        "    \n",
        "    return train_dataset\n",
        "\n",
        "## Define function to prepare testing dataset\n",
        "\n",
        "def calculate_mean(table):\n",
        "    values = table.values()\n",
        "    return sum(values)/(len(values))\n",
        "\n",
        "def add_feature_history_to_test(test_dataset, col, proportion_history, total_comments_history):\n",
        "    default_proportion = calculate_mean(proportion_history)\n",
        "    default_total_comments = calculate_mean(total_comments_history)\n",
        "    \n",
        "    def getProportion(col_val):\n",
        "        proportion = default_proportion\n",
        "        if col_val in proportion_history:\n",
        "            proportion = proportion_history[col_val]\n",
        "    \n",
        "        return proportion\n",
        "    \n",
        "    def getTotal(col_val):\n",
        "        total = default_total_comments\n",
        "        if col_val in total_comments_history:\n",
        "            total = total_comments_history[col_val]\n",
        "        \n",
        "        return total\n",
        "    \n",
        "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
        "    total_col = \"total_num_comments_by_\" + col\n",
        "    \n",
        "    test_dataset[proportion_col] = test_dataset[col].apply(lambda x: getProportion(x))\n",
        "    test_dataset[total_col] = test_dataset[col].apply(lambda x: getTotal(x))\n",
        "    \n",
        "    return test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd1aVQRl5N2E"
      },
      "source": [
        "### Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiHzLU8BNdCJ",
        "tags": []
      },
      "source": [
        "import re\n",
        "def remove_punctuation(text):\n",
        "    text_without_punctuation = re.sub(r'[^\\w\\s]', '',  text)\n",
        "    return text_without_punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7mP72YW5N2F"
      },
      "source": [
        "def lemmatize(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LwY1QHRQue8"
      },
      "source": [
        "### Lowercasing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9QMjyYLQwsx",
        "tags": []
      },
      "source": [
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUjaFdZeQ54S"
      },
      "source": [
        "### Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OdVgxBTQ83A",
        "tags": []
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "def remove_stop_words(tokens): \n",
        "    return [word for word in tokens if word not in stoplist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bcCGhgQP5N2G",
        "outputId": "e859bb03-fa56-4313-ec8c-898bcc9731de"
      },
      "source": [
        "def change_data(isRemovePunctuation, isLowerCase, isRemoveStopWords, data, isParent):\n",
        "    cleaned = \"comment_cleaned\"\n",
        "    processed = \"processed_comment\"\n",
        "    comment = \"comment\"\n",
        "    tokens_word = \"tokens\"\n",
        "    if isParent:\n",
        "        cleaned = \"parent_\" + cleaned\n",
        "        processed = \"parent_\" + processed\n",
        "        comment = \"parent_\" + comment\n",
        "        tokens_word = \"parent_\" + tokens_word\n",
        "        \n",
        "    if isRemovePunctuation:\n",
        "        data[cleaned] = data[comment].apply(lambda x: remove_punctuation(x))\n",
        "    else:\n",
        "        data[cleaned] = data[comment]\n",
        "    print(\"punctuation\")    \n",
        "    tokens = [word_tokenize(sen) for sen in data[cleaned]]\n",
        "    tokens = [lemmatize(words) for words in tokens]\n",
        "    if isLowerCase: \n",
        "        adapted_tokens = [lower_token(token) for token in tokens]\n",
        "    else:\n",
        "        adapted_tokens = tokens\n",
        "    print(\"lowercase\")\n",
        "    if isRemoveStopWords:\n",
        "        filtered_words = [remove_stop_words(sen) for sen in adapted_tokens]\n",
        "        data[processed] = [' '.join(token) for token in filtered_words]\n",
        "        data[tokens_word] = filtered_words\n",
        "    else:    \n",
        "        data[processed] = [' '.join(token) for token in adapted_tokens]\n",
        "        print(\"processed\")\n",
        "        data[tokens_word] = adapted_tokens\n",
        "    print(\"stopwords\")    \n",
        "    return data, tokens\n",
        "\n",
        "data_cnn, tokens = change_data(True, True, False, data_cnn, False)\n",
        "data_cnn, parent_tokens = change_data(True, True, False, data_cnn, True)\n",
        "data_cnn.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "punctuation\n",
            "lowercase\n",
            "processed\n",
            "stopwords\n",
            "punctuation\n",
            "lowercase\n",
            "processed\n",
            "stopwords\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c_neg</th>\n",
              "      <th>c_neu</th>\n",
              "      <th>c_pos</th>\n",
              "      <th>pc_neg</th>\n",
              "      <th>pc_neu</th>\n",
              "      <th>pc_pos</th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>...</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment_cleaned</th>\n",
              "      <th>processed_comment</th>\n",
              "      <th>tokens</th>\n",
              "      <th>parent_comment_cleaned</th>\n",
              "      <th>parent_processed_comment</th>\n",
              "      <th>parent_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.125524</td>\n",
              "      <td>0.740467</td>\n",
              "      <td>0.134010</td>\n",
              "      <td>0.208965</td>\n",
              "      <td>0.730244</td>\n",
              "      <td>0.060791</td>\n",
              "      <td>0</td>\n",
              "      <td>NC and NH.</td>\n",
              "      <td>Trumpbart</td>\n",
              "      <td>politics</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-16 23:55:23</td>\n",
              "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
              "      <td>NC and NH</td>\n",
              "      <td>nc and nh</td>\n",
              "      <td>[nc, and, nh]</td>\n",
              "      <td>Yeah I get that argument At this point Id pref...</td>\n",
              "      <td>yeah i get that argument at this point id pref...</td>\n",
              "      <td>[yeah, i, get, that, argument, at, this, point...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.155209</td>\n",
              "      <td>0.810037</td>\n",
              "      <td>0.034754</td>\n",
              "      <td>0.754650</td>\n",
              "      <td>0.222954</td>\n",
              "      <td>0.022396</td>\n",
              "      <td>0</td>\n",
              "      <td>You do know west teams play against west teams...</td>\n",
              "      <td>Shbshb906</td>\n",
              "      <td>nba</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-01 00:24:10</td>\n",
              "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
              "      <td>You do know west teams play against west teams...</td>\n",
              "      <td>you do know west team play against west team m...</td>\n",
              "      <td>[you, do, know, west, team, play, against, wes...</td>\n",
              "      <td>The blazers and Mavericks The wests 5 and 6 se...</td>\n",
              "      <td>the blazer and mavericks the west 5 and 6 seed...</td>\n",
              "      <td>[the, blazer, and, mavericks, the, west, 5, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.071656</td>\n",
              "      <td>0.868689</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>0.007329</td>\n",
              "      <td>0.296819</td>\n",
              "      <td>0.695852</td>\n",
              "      <td>0</td>\n",
              "      <td>They were underdogs earlier today, but since G...</td>\n",
              "      <td>Creepeth</td>\n",
              "      <td>nfl</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-22 21:45:37</td>\n",
              "      <td>They're favored to win.</td>\n",
              "      <td>They were underdogs earlier today but since Gr...</td>\n",
              "      <td>they were underdog earlier today but since gro...</td>\n",
              "      <td>[they, were, underdog, earlier, today, but, si...</td>\n",
              "      <td>Theyre favored to win</td>\n",
              "      <td>theyre favored to win</td>\n",
              "      <td>[theyre, favored, to, win]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.923330</td>\n",
              "      <td>0.069511</td>\n",
              "      <td>0.007159</td>\n",
              "      <td>0.633449</td>\n",
              "      <td>0.327936</td>\n",
              "      <td>0.038615</td>\n",
              "      <td>0</td>\n",
              "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
              "      <td>icebrotha</td>\n",
              "      <td>BlackPeopleTwitter</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-18 21:03:47</td>\n",
              "      <td>deadass don't kill my buzz</td>\n",
              "      <td>This meme isnt funny none of the new york nigg...</td>\n",
              "      <td>this meme isnt funny none of the new york nigg...</td>\n",
              "      <td>[this, meme, isnt, funny, none, of, the, new, ...</td>\n",
              "      <td>deadass dont kill my buzz</td>\n",
              "      <td>deadass dont kill my buzz</td>\n",
              "      <td>[deadass, dont, kill, my, buzz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.065801</td>\n",
              "      <td>0.661140</td>\n",
              "      <td>0.273059</td>\n",
              "      <td>0.067330</td>\n",
              "      <td>0.776531</td>\n",
              "      <td>0.156140</td>\n",
              "      <td>0</td>\n",
              "      <td>I could use one of those tools.</td>\n",
              "      <td>cush2push</td>\n",
              "      <td>MaddenUltimateTeam</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-12</td>\n",
              "      <td>2016-12-30 17:00:13</td>\n",
              "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
              "      <td>I could use one of those tools</td>\n",
              "      <td>i could use one of those tool</td>\n",
              "      <td>[i, could, use, one, of, those, tool]</td>\n",
              "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
              "      <td>yep can confirm i saw the tool they use for th...</td>\n",
              "      <td>[yep, can, confirm, i, saw, the, tool, they, u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      c_neg     c_neu     c_pos    pc_neg    pc_neu    pc_pos  label  \\\n",
              "0  0.125524  0.740467  0.134010  0.208965  0.730244  0.060791      0   \n",
              "1  0.155209  0.810037  0.034754  0.754650  0.222954  0.022396      0   \n",
              "2  0.071656  0.868689  0.059655  0.007329  0.296819  0.695852      0   \n",
              "3  0.923330  0.069511  0.007159  0.633449  0.327936  0.038615      0   \n",
              "4  0.065801  0.661140  0.273059  0.067330  0.776531  0.156140      0   \n",
              "\n",
              "                                             comment     author  \\\n",
              "0                                         NC and NH.  Trumpbart   \n",
              "1  You do know west teams play against west teams...  Shbshb906   \n",
              "2  They were underdogs earlier today, but since G...   Creepeth   \n",
              "3  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
              "4                    I could use one of those tools.  cush2push   \n",
              "\n",
              "            subreddit  ...  downs     date          created_utc  \\\n",
              "0            politics  ...     -1  2016-10  2016-10-16 23:55:23   \n",
              "1                 nba  ...     -1  2016-11  2016-11-01 00:24:10   \n",
              "2                 nfl  ...      0  2016-09  2016-09-22 21:45:37   \n",
              "3  BlackPeopleTwitter  ...     -1  2016-10  2016-10-18 21:03:47   \n",
              "4  MaddenUltimateTeam  ...     -1  2016-12  2016-12-30 17:00:13   \n",
              "\n",
              "                                      parent_comment  \\\n",
              "0  Yeah, I get that argument. At this point, I'd ...   \n",
              "1  The blazers and Mavericks (The wests 5 and 6 s...   \n",
              "2                            They're favored to win.   \n",
              "3                         deadass don't kill my buzz   \n",
              "4  Yep can confirm I saw the tool they use for th...   \n",
              "\n",
              "                                     comment_cleaned  \\\n",
              "0                                          NC and NH   \n",
              "1  You do know west teams play against west teams...   \n",
              "2  They were underdogs earlier today but since Gr...   \n",
              "3  This meme isnt funny none of the new york nigg...   \n",
              "4                     I could use one of those tools   \n",
              "\n",
              "                                   processed_comment  \\\n",
              "0                                          nc and nh   \n",
              "1  you do know west team play against west team m...   \n",
              "2  they were underdog earlier today but since gro...   \n",
              "3  this meme isnt funny none of the new york nigg...   \n",
              "4                      i could use one of those tool   \n",
              "\n",
              "                                              tokens  \\\n",
              "0                                      [nc, and, nh]   \n",
              "1  [you, do, know, west, team, play, against, wes...   \n",
              "2  [they, were, underdog, earlier, today, but, si...   \n",
              "3  [this, meme, isnt, funny, none, of, the, new, ...   \n",
              "4              [i, could, use, one, of, those, tool]   \n",
              "\n",
              "                              parent_comment_cleaned  \\\n",
              "0  Yeah I get that argument At this point Id pref...   \n",
              "1  The blazers and Mavericks The wests 5 and 6 se...   \n",
              "2                              Theyre favored to win   \n",
              "3                          deadass dont kill my buzz   \n",
              "4  Yep can confirm I saw the tool they use for th...   \n",
              "\n",
              "                            parent_processed_comment  \\\n",
              "0  yeah i get that argument at this point id pref...   \n",
              "1  the blazer and mavericks the west 5 and 6 seed...   \n",
              "2                              theyre favored to win   \n",
              "3                          deadass dont kill my buzz   \n",
              "4  yep can confirm i saw the tool they use for th...   \n",
              "\n",
              "                                       parent_tokens  \n",
              "0  [yeah, i, get, that, argument, at, this, point...  \n",
              "1  [the, blazer, and, mavericks, the, west, 5, an...  \n",
              "2                         [theyre, favored, to, win]  \n",
              "3                    [deadass, dont, kill, my, buzz]  \n",
              "4  [yep, can, confirm, i, saw, the, tool, they, u...  \n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqL8EUxsRWBP",
        "tags": []
      },
      "source": [
        "def make_processed_dataset(data, isParent):\n",
        "    sarcasm = []\n",
        "    non_sarcasm = []\n",
        "    for l in data.label:\n",
        "        if l == 0:\n",
        "            sarcasm.append(0)\n",
        "            non_sarcasm.append(1)\n",
        "        elif l == 1:\n",
        "            sarcasm.append(1)\n",
        "            non_sarcasm.append(0)\n",
        "    data['sarcasm']= sarcasm\n",
        "    data['non_sarcasm']= non_sarcasm\n",
        "    if isParent:\n",
        "        data_processed = data[['processed_comment', 'parent_processed_comment', 'tokens', 'parent_tokens', 'label', 'sarcasm', 'non_sarcasm', 'subreddit', \n",
        "                               'c_pos', 'c_neu', 'c_neg', 'pc_pos', 'pc_neu', 'pc_neg']]\n",
        "    else:    \n",
        "        data_processed = data[['processed_comment', 'tokens', 'label', 'sarcasm', 'non_sarcasm', 'subreddit', 'c_pos', 'c_neu',\n",
        "                              'c_neg', 'pc_pos', 'pc_neu', 'pc_neg']]\n",
        "    return data_processed\n",
        "\n",
        "\n",
        "data_cnn_processed = make_processed_dataset(data_cnn, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8fEd6VL0YuZ"
      },
      "source": [
        "### Split into training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGuYY9tXNmtd",
        "tags": []
      },
      "source": [
        "def split_train_test(data_processed):\n",
        "    data_train, data_test = train_test_split(\n",
        "    data_processed,\n",
        "    test_size = 0.25,\n",
        "    random_state = 1000)\n",
        "    return data_train, data_test\n",
        "\n",
        "data_cnn_train, data_cnn_test = split_train_test(data_cnn_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOVdZUXv5N2H"
      },
      "source": [
        "SUBREDDIT = \"subreddit\"\n",
        "data_cnn_train = add_feature_history_to_train(\n",
        "    data_cnn_train, SUBREDDIT)\n",
        "proportion_history_subreddit, total_comments_history_subreddit = feature_history(\n",
        "    data_cnn_train, SUBREDDIT)\n",
        "data_cnn_test = add_feature_history_to_test(\n",
        "    data_cnn_test, SUBREDDIT,\n",
        "    proportion_history_subreddit, total_comments_history_subreddit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92-gA7JD5N2H",
        "outputId": "0c80a2bd-ae3e-4916-bee4-1ad950f1c420"
      },
      "source": [
        "print(data_cnn_train.columns.get_loc(\"total_num_comments_by_subreddit\"))\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler() \n",
        "scaled_values = scaler.fit_transform(data_cnn_train.iloc[:, 8:])\n",
        "data_cnn_train.iloc[:,8:] = scaled_values\n",
        "\n",
        "scaled_values = scaler.fit_transform(data_cnn_test.iloc[:, 8:]) \n",
        "data_cnn_test.iloc[:,8:] = scaled_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlBnz24y5N2I"
      },
      "source": [
        "data_cnn_train[['c_pos', 'c_neu', 'c_neg', 'pc_pos', 'pc_neu', 'pc_neg', 'sarcasm_proportion_by_subreddit', \n",
        "                'total_num_comments_by_subreddit']] = data_cnn_train[['c_pos', 'c_neu', 'c_neg', 'pc_pos', 'pc_neu', 'pc_neg', 'sarcasm_proportion_by_subreddit', \n",
        "                'total_num_comments_by_subreddit']].apply(lambda x: x/100)\n",
        "data_cnn_test[['c_pos', 'c_neu', 'c_neg', 'pc_pos', 'pc_neu', 'pc_neg', 'sarcasm_proportion_by_subreddit', \n",
        "                'total_num_comments_by_subreddit']] = data_cnn_test[['c_pos', 'c_neu', 'c_neg', 'pc_pos', 'pc_neu', 'pc_neg', 'sarcasm_proportion_by_subreddit', \n",
        "                'total_num_comments_by_subreddit']].apply(lambda x: x/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nQPWaXk5N2I",
        "outputId": "82ed0539-e534-4cbb-9303-22ff6891061c"
      },
      "source": [
        "print(data_cnn_train.shape)\n",
        "print(data_cnn_test.shape)\n",
        "data_cnn_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(758119, 16)\n",
            "(252707, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_comment</th>\n",
              "      <th>parent_processed_comment</th>\n",
              "      <th>tokens</th>\n",
              "      <th>parent_tokens</th>\n",
              "      <th>label</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>non_sarcasm</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>c_pos</th>\n",
              "      <th>c_neu</th>\n",
              "      <th>c_neg</th>\n",
              "      <th>pc_pos</th>\n",
              "      <th>pc_neu</th>\n",
              "      <th>pc_neg</th>\n",
              "      <th>sarcasm_proportion_by_subreddit</th>\n",
              "      <th>total_num_comments_by_subreddit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>987945</th>\n",
              "      <td>this is not fucking foreshadowing</td>\n",
              "      <td>re watching superbad and found a bit of foresh...</td>\n",
              "      <td>[this, is, not, fucking, foreshadowing]</td>\n",
              "      <td>[re, watching, superbad, and, found, a, bit, o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>movies</td>\n",
              "      <td>0.003025</td>\n",
              "      <td>0.020896</td>\n",
              "      <td>0.986569</td>\n",
              "      <td>0.070298</td>\n",
              "      <td>0.544439</td>\n",
              "      <td>0.405272</td>\n",
              "      <td>0.483779</td>\n",
              "      <td>0.100207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591874</th>\n",
              "      <td>yeah the fact he doe it every single league on...</td>\n",
              "      <td>poe community is one of the worst i have ever ...</td>\n",
              "      <td>[yeah, the, fact, he, doe, it, every, single, ...</td>\n",
              "      <td>[poe, community, is, one, of, the, worst, i, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>pathofexile</td>\n",
              "      <td>0.063181</td>\n",
              "      <td>0.579354</td>\n",
              "      <td>0.379834</td>\n",
              "      <td>0.004294</td>\n",
              "      <td>0.043181</td>\n",
              "      <td>0.962369</td>\n",
              "      <td>0.483137</td>\n",
              "      <td>0.024081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260829</th>\n",
              "      <td>left wing inspector coming thru</td>\n",
              "      <td>another day in tier 1</td>\n",
              "      <td>[left, wing, inspector, coming, thru]</td>\n",
              "      <td>[another, day, in, tier, 1]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Warthunder</td>\n",
              "      <td>0.069767</td>\n",
              "      <td>0.911826</td>\n",
              "      <td>0.048112</td>\n",
              "      <td>0.110416</td>\n",
              "      <td>0.802874</td>\n",
              "      <td>0.111909</td>\n",
              "      <td>0.530176</td>\n",
              "      <td>0.021866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98631</th>\n",
              "      <td>dear lord</td>\n",
              "      <td>they preordered tgt pack</td>\n",
              "      <td>[dear, lord]</td>\n",
              "      <td>[they, preordered, tgt, pack]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>hearthstone</td>\n",
              "      <td>0.178047</td>\n",
              "      <td>0.661987</td>\n",
              "      <td>0.183138</td>\n",
              "      <td>0.065460</td>\n",
              "      <td>0.900080</td>\n",
              "      <td>0.062097</td>\n",
              "      <td>0.520276</td>\n",
              "      <td>0.067631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232927</th>\n",
              "      <td>shocking that the middle of nowhere would rank...</td>\n",
              "      <td>alberta would rank 4th in global quality of li...</td>\n",
              "      <td>[shocking, that, the, middle, of, nowhere, wou...</td>\n",
              "      <td>[alberta, would, rank, 4th, in, global, qualit...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>canada</td>\n",
              "      <td>0.008718</td>\n",
              "      <td>0.137173</td>\n",
              "      <td>0.867134</td>\n",
              "      <td>0.422987</td>\n",
              "      <td>0.585976</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>0.597545</td>\n",
              "      <td>0.061230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        processed_comment  \\\n",
              "987945                  this is not fucking foreshadowing   \n",
              "591874  yeah the fact he doe it every single league on...   \n",
              "260829                    left wing inspector coming thru   \n",
              "98631                                           dear lord   \n",
              "232927  shocking that the middle of nowhere would rank...   \n",
              "\n",
              "                                 parent_processed_comment  \\\n",
              "987945  re watching superbad and found a bit of foresh...   \n",
              "591874  poe community is one of the worst i have ever ...   \n",
              "260829                              another day in tier 1   \n",
              "98631                            they preordered tgt pack   \n",
              "232927  alberta would rank 4th in global quality of li...   \n",
              "\n",
              "                                                   tokens  \\\n",
              "987945            [this, is, not, fucking, foreshadowing]   \n",
              "591874  [yeah, the, fact, he, doe, it, every, single, ...   \n",
              "260829              [left, wing, inspector, coming, thru]   \n",
              "98631                                        [dear, lord]   \n",
              "232927  [shocking, that, the, middle, of, nowhere, wou...   \n",
              "\n",
              "                                            parent_tokens  label  sarcasm  \\\n",
              "987945  [re, watching, superbad, and, found, a, bit, o...      0        0   \n",
              "591874  [poe, community, is, one, of, the, worst, i, h...      1        1   \n",
              "260829                        [another, day, in, tier, 1]      0        0   \n",
              "98631                       [they, preordered, tgt, pack]      0        0   \n",
              "232927  [alberta, would, rank, 4th, in, global, qualit...      1        1   \n",
              "\n",
              "        non_sarcasm    subreddit     c_pos     c_neu     c_neg    pc_pos  \\\n",
              "987945            1       movies  0.003025  0.020896  0.986569  0.070298   \n",
              "591874            0  pathofexile  0.063181  0.579354  0.379834  0.004294   \n",
              "260829            1   Warthunder  0.069767  0.911826  0.048112  0.110416   \n",
              "98631             1  hearthstone  0.178047  0.661987  0.183138  0.065460   \n",
              "232927            0       canada  0.008718  0.137173  0.867134  0.422987   \n",
              "\n",
              "          pc_neu    pc_neg  sarcasm_proportion_by_subreddit  \\\n",
              "987945  0.544439  0.405272                         0.483779   \n",
              "591874  0.043181  0.962369                         0.483137   \n",
              "260829  0.802874  0.111909                         0.530176   \n",
              "98631   0.900080  0.062097                         0.520276   \n",
              "232927  0.585976  0.009083                         0.597545   \n",
              "\n",
              "        total_num_comments_by_subreddit  \n",
              "987945                         0.100207  \n",
              "591874                         0.024081  \n",
              "260829                         0.021866  \n",
              "98631                          0.067631  \n",
              "232927                         0.061230  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyvwQ_3iDw1h"
      },
      "source": [
        "## Download Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "FljVP87k5N2J"
      },
      "source": [
        "word2vec = gensim.downloader.load('glove-twitter-200')\n",
        "##word2vec = gensim.downloader.load('glove-twitter-100')\n",
        "##word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
        "##wiki_word2vec = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "##word2vec = Word2Vec(sentences=data_cnn_train[\"tokens\"], vector_size=100, window=5, min_count=1, workers=4).wv\n",
        "##word2vec = Word2Vec(sentences=data_cnn_train[\"tokens\"], vector_size=200, window=5, min_count=1, workers=4).wv\n",
        "##word2vec = Word2Vec(sentences=data_cnn_train[\"tokens\"], vector_size=300, window=5, min_count=1, workers=4).wv\n",
        "parent_word2vec = word2vec\n",
        "##parent_word2vec = Word2Vec(sentences=data_cnn_train[\"parent_tokens\"], vector_size=300, window=5, min_count=1, workers=4).wv\n",
        "##word2vec = Word2Vec(sentences=data_cnn_train[\"tokens\"], vector_size=400, window=5, min_count=1, workers=4).wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HzPAPxg53hq"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Yi_XK150aB",
        "tags": []
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 200\n",
        "EMBEDDING_DIM = 200\n",
        "num_epochs = 10\n",
        "batch_size = 34\n",
        "NUM_FILTERS = 300\n",
        "PARENT_NUM_FILTERS = 100\n",
        "FILTER_SIZES = [2,3,4]\n",
        "PARENT_FILTER_SIZES = [2,3,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGDwl4-5N2K"
      },
      "source": [
        "### Vocab Assembly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MopdnqKK0O6P",
        "tags": [],
        "outputId": "9931419f-4706-457a-cf81-7a7d2a5e6bdd"
      },
      "source": [
        "all_training_words = [word for tokens in data_cnn_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_cnn_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7909196 words total, with a vocabulary size of 169575\n",
            "Max sentence length is 2222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5OAiGdc1ZW7",
        "tags": [],
        "outputId": "9c6735fe-accf-4c55-ce78-921a157158c7"
      },
      "source": [
        "all_test_words = [word for tokens in data_cnn_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_cnn_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2634403 words total, with a vocabulary size of 87612\n",
            "Max sentence length is 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0khyn7n5N2K"
      },
      "source": [
        "### Parent Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "_g6NOsvk5N2K",
        "outputId": "46d99ab1-c8a1-4722-d3be-0a9d5b6de221"
      },
      "source": [
        "parent_all_training_words = [word for parent_tokens in data_cnn_train[\"parent_tokens\"] for word in parent_tokens]\n",
        "parent_training_sentence_lengths = [len(parent_tokens) for parent_tokens in data_cnn_train[\"parent_tokens\"]]\n",
        "PARENT_TRAINING_VOCAB = sorted(list(set(parent_all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(parent_all_training_words), len(PARENT_TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(parent_training_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18409070 words total, with a vocabulary size of 262366\n",
            "Max sentence length is 4198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "gK-pQ9UG5N2L",
        "outputId": "85266da3-9e3e-4f8b-9dc0-99bcccfda5f1"
      },
      "source": [
        "parent_all_test_words = [word for parent_tokens in data_cnn_test[\"parent_tokens\"] for word in parent_tokens]\n",
        "parent_test_sentence_lengths = [len(parent_tokens) for parent_tokens in data_cnn_test[\"parent_tokens\"]]\n",
        "PARENT_TEST_VOCAB = sorted(list(set(parent_all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(parent_all_test_words), len(PARENT_TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(parent_test_sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6144465 words total, with a vocabulary size of 134517\n",
            "Max sentence length is 3055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0N9NThg5N2L"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW4MdlCP2BT2",
        "tags": [],
        "outputId": "aa92c689-b6f1-40d4-b6fc-32629ef992db"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_cnn_train[\"processed_comment\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_cnn_train[\"processed_comment\"].tolist())\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))\n",
        "data_cnn_train_final = pad_sequences(training_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_cnn_train_final = np.ndarray.astype(data_cnn_train_final, dtype=\"float64\")\n",
        "sentiment_train = data_cnn_train[[\"c_pos\", \"c_neu\", \"c_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy()\n",
        "#sentiment_train = np.zeros(data_cnn_train[[\"c_pos\", \"c_neu\", \"c_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy().shape)\n",
        "data_cnn_train_final = np.append(data_cnn_train_final, sentiment_train, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 168893 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11gISiTM4pu0",
        "tags": []
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_cnn_test[\"processed_comment\"].tolist())\n",
        "data_cnn_test_final = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "data_cnn_test_final = np.ndarray.astype(data_cnn_test_final, dtype=\"float64\")\n",
        "\n",
        "sentiment_test = data_cnn_test[[\"c_pos\", \"c_neu\", \"c_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy()\n",
        "#sentiment_test = np.zeros(data_cnn_test[[\"c_pos\", \"c_neu\", \"c_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy().shape)\n",
        "data_cnn_test_final = np.append(data_cnn_test_final, sentiment_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCT45BIF5N2M"
      },
      "source": [
        "### Parent Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QV4gL00K5N2M",
        "outputId": "c6db1861-4fda-4243-9fb3-6798afc0c151"
      },
      "source": [
        "parent_tokenizer = Tokenizer(num_words=len(PARENT_TRAINING_VOCAB), lower=True, char_level=False)\n",
        "parent_tokenizer.fit_on_texts(data_cnn_train[\"parent_processed_comment\"].tolist())\n",
        "parent_training_sequences = parent_tokenizer.texts_to_sequences(data_cnn_train[\"parent_processed_comment\"].tolist())\n",
        "parent_train_word_index = parent_tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(parent_train_word_index))\n",
        "parent_data_cnn_train_final = pad_sequences(parent_training_sequences, \n",
        "                               maxlen=MAX_SEQUENCE_LENGTH)\n",
        "parent_data_cnn_train_final = np.ndarray.astype(parent_data_cnn_train_final, dtype=\"float64\")\n",
        "parent_sentiment_train = data_cnn_train[[\"pc_pos\", \"pc_neu\", \"pc_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy()\n",
        "#parent_sentiment_train = np.zeros(data_cnn_train[[\"pc_pos\", \"pc_neu\", \"pc_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy().shape)\n",
        "parent_data_cnn_train_final = np.append(parent_data_cnn_train_final, parent_sentiment_train, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 261376 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "yu6xiTVx5N2M"
      },
      "source": [
        "parent_test_sequences = parent_tokenizer.texts_to_sequences(data_cnn_test[\"parent_processed_comment\"].tolist())\n",
        "parent_data_cnn_test_final = pad_sequences(parent_test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "parent_data_cnn_test_final = np.ndarray.astype(parent_data_cnn_test_final, dtype=\"float64\")\n",
        "parent_sentiment_test = data_cnn_test[[\"pc_pos\", \"pc_neu\", \"pc_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy()\n",
        "#parent_sentiment_test = np.zeros(data_cnn_test[[\"pc_pos\", \"pc_neu\", \"pc_neg\", \"sarcasm_proportion_by_subreddit\", \"total_num_comments_by_subreddit\"]].to_numpy().shape)\n",
        "parent_data_cnn_test_final = np.append(parent_data_cnn_test_final, parent_sentiment_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws6tIliO5N2M"
      },
      "source": [
        "### End of Parent Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ru6A5qC5SaK",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "6297a379-bf94-44a0-9690-798413b52f6f"
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
        " EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        " train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-00a4bc4a90c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_embedding_weights = np.zeros((len(train_word_index)+1, \n\u001b[0m\u001b[1;32m      2\u001b[0m  EMBEDDING_DIM))\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_word_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0mtrain_embedding_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embedding_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "9S8uHOAl5N2M"
      },
      "source": [
        "parent_train_embedding_weights = np.zeros((len(parent_train_word_index)+1, \n",
        " EMBEDDING_DIM))\n",
        "for word,index in parent_train_word_index.items():\n",
        " parent_train_embedding_weights[index,:] = parent_word2vec[word] if word in parent_word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(parent_train_embedding_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "K1ePGS9l5N2N"
      },
      "source": [
        "def Parent_Convolution_Net_Alt5(embeddings, parent_embeddings, max_sequence_length, num_words, parent_num_words, \n",
        "                           embedding_dim, labels_index, filters, parent_filters, \n",
        "                           filter_sizes, parent_filter_sizes):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    parent_embedding_layer = Embedding(parent_num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[parent_embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(2, max_sequence_length+5))\n",
        "    comment_sequence_input, parent_sequence_input = tf.split(sequence_input, 2, 1)\n",
        "    comment_sequence_input = tf.squeeze(comment_sequence_input, axis=1)\n",
        "    parent_sequence_input = tf.squeeze(parent_sequence_input, axis=1)\n",
        "    \n",
        "    comment_sequence_input, non_text_comment_input = tf.split(comment_sequence_input, [200,5], 1)\n",
        "    comment_sequence_input = tf.cast(comment_sequence_input, dtype=\"int64\")\n",
        "    parent_sequence_input, non_text_parent_input, irrelevant_input = tf.split(parent_sequence_input, [200,3,2], 1)\n",
        "    parent_sequence_input = tf.cast(parent_sequence_input, dtype=\"int64\")\n",
        "    \n",
        "    embedded_sequences = embedding_layer(comment_sequence_input)\n",
        "    parent_embedded_sequences = parent_embedding_layer(parent_sequence_input)\n",
        "\n",
        "    convs = []\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=filters, kernel_size=filter_size, \n",
        "                        #kernel_initializer='random_normal', bias_initializer='zeros', \n",
        "                        #kernel_regularizer=regularizers.l1_l2(l1=1e-6, l2=1e-6), \n",
        "                        #bias_regularizer=regularizers.l2(1e-6), activity_regularizer=regularizers.l2(1e-6), \n",
        "                        activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "        \n",
        "    for filter_size in parent_filter_sizes:\n",
        "        parent_l_conv = Conv1D(filters=parent_filters, kernel_size=filter_size, \n",
        "                               #kernel_initializer='zeros', \n",
        "                               #bias_initializer='zeros', \n",
        "                               #kernel_regularizer=regularizers.l1_l2(l1=1e-6, l2=1e-6), \n",
        "                               #bias_regularizer=regularizers.l2(1e-6), activity_regularizer=regularizers.l2(1e-6), \n",
        "                               activation='relu')(parent_embedded_sequences)\n",
        "        parent_l_pool = GlobalMaxPooling1D()(parent_l_conv)\n",
        "        convs.append(parent_l_pool)  \n",
        "        \n",
        "    #convs.append(non_text_comment_input)\n",
        "    #convs.append(non_text_parent_input)\n",
        "    \n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "    ## best 128,64\n",
        "    x = Dropout(0.40)(l_merge)  \n",
        "    x = Dense(128, \n",
        "              #kernel_initializer='zeros', \n",
        "              #bias_initializer='random_normal', kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3), \n",
        "              #bias_regularizer=regularizers.l2(1e-3), activity_regularizer=regularizers.l2(1e-3), \n",
        "              activation='relu')(x)\n",
        "    x = Dense(64, \n",
        "              #kernel_initializer='zeros', \n",
        "              #bias_initializer='random_normal', kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3), \n",
        "              #bias_regularizer=regularizers.l2(1e-3), activity_regularizer=regularizers.l2(1e-3), \n",
        "              activation='relu')(x)\n",
        "    #x = concatenate([non_text_comment_input, non_text_parent_input, x], axis=1)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = concatenate([non_text_comment_input, non_text_parent_input, x], axis=1)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "    adam_optimizer = RMSprop(learning_rate=0.0001, momentum=0.5, clipnorm=0.1, clipvalue=0.1)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam_optimizer,\n",
        "                  metrics=['acc'])\n",
        "    #model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bKxo1uU6dFJ",
        "tags": []
      },
      "source": [
        "label_names = ['sarcasm', 'non_sarcasm']\n",
        "labels = [1, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r6cCAIv6j_P",
        "tags": []
      },
      "source": [
        "y_train = data_cnn_train[label_names].values\n",
        "##x_train = data_cnn_train_final\n",
        "x_train = np.swapaxes(np.array([data_cnn_train_final, parent_data_cnn_train_final]), 0, 1)\n",
        "y_tr = y_train\n",
        "print(x_train.shape)\n",
        "##x_train.to_csv(r'x_train_glove2000.csv')\n",
        "##y_train.to_csv(r'y_train_glove2000.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eIefkiB5N2O"
      },
      "source": [
        "data_cnn_train_final[:5, -5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU0br7Pp5N2P"
      },
      "source": [
        "model_alt = Parent_Convolution_Net_Alt5(train_embedding_weights, parent_train_embedding_weights, \n",
        "                               MAX_SEQUENCE_LENGTH, len(train_word_index)+1, \n",
        "                        len(parent_train_word_index)+1, EMBEDDING_DIM, \n",
        "                        len(list(label_names)), NUM_FILTERS, PARENT_NUM_FILTERS, \n",
        "                               FILTER_SIZES, PARENT_FILTER_SIZES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0JWL_S35N2P"
      },
      "source": [
        "hist = model_alt.fit(x_train, y_tr, epochs=1, validation_split=0.1, shuffle=True, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4LeLb8Z5N2P"
      },
      "source": [
        "# evaluate the model\n",
        "x_test = np.swapaxes(np.array([data_cnn_test_final, parent_data_cnn_test_final]), 0, 1)\n",
        "y_test = data_cnn_test.label\n",
        "\n",
        "train_err = model_alt.evaluate(x_train, y_tr, verbose=0)\n",
        "test_err = model_alt.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6mlFIlJ5N2P"
      },
      "source": [
        "# plot loss during training\n",
        "pyplot.title('Binary CrossEntropy')\n",
        "pyplot.plot(hist.history['loss'], label='train')\n",
        "pyplot.plot(hist.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qOXOLLu_xQp"
      },
      "source": [
        "predictions_test = model_alt.predict(np.swapaxes(np.array([data_cnn_test_final, parent_data_cnn_test_final]),\n",
        "                                             0, 1), batch_size=1024, verbose=1)\n",
        "#predictions_train = model.predict(x_train, batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgNwxY9V_3t7"
      },
      "source": [
        "prediction_labels_test =[]\n",
        "for p in predictions_test:\n",
        "    prediction_labels_test.append(labels[np.argmax(p)])\n",
        "    \n",
        "#prediction_labels_train =[]\n",
        "#for p in predictions_train:\n",
        "    #prediction_labels_train.append(labels[np.argmax(p)])\n",
        "    \n",
        "print(data_cnn_test.shape)\n",
        "print(len(prediction_labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG41DySTADEi"
      },
      "source": [
        "print(sum(data_cnn_test.label==prediction_labels_test)/len(prediction_labels_test))\n",
        "#print(sum(data_cnn_train.label==prediction_labels_train)/len(prediction_labels_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "-0fgUSQW5N2Q"
      },
      "source": [
        "model.save(\"twitter200_200dim_200len_3.1_epochs_parent_300_100_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaGIgUPP5N2Q"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f1_score(data_cnn_test.label, prediction_labels_test))\n",
        "mat = confusion_matrix(data_cnn_test.label, prediction_labels_test)\n",
        "disp = ConfusionMatrixDisplay(mat)\n",
        "disp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "H6rPZT1U5N2Q"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"twitter200_200dim_200len_3_epochs_parent_300_100_model.h5\")\n",
        "model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbDX38E3AGrd"
      },
      "source": [
        "print(data_cnn_test.label.value_counts())\n",
        "print(data_cnn_train.label.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y89NS1xAP4h"
      },
      "source": [
        "model_alt.get_weights()[18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kumqgM7l5N2Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}